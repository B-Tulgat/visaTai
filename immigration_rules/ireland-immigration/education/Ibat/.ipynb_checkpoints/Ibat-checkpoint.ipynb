{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b4b7075-c77d-4a94-a212-7d66f9b61410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/amon/visa-ai/immigration_rules/ireland-immigration/education/Ibat\n"
     ]
    }
   ],
   "source": [
    "https://www.ibat.ie/studying-at-ibat/international-students"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27e62645-1361-4cd2-b9e3-fd3ff1f90426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PAGE] https://www.ibat.ie/english-language-courses/learn-general-english/ -> ./english_language_courses/learn_general_english.txt\n",
      "[PAGE] https://www.ibat.ie/english-language-courses/ielts-prep -> ./english_language_courses/ielts_prep.txt\n",
      "[DOC] https://www.ibat.ie/wp-content/uploads/2024/11/v4-IBAT-077-Brochure_English-Language-Factsheet.pdf -> ./wp_content/uploads/2024/11/v4_IBAT_077_Brochure_English_Language_Factsheet.pdf\n",
      "[DOC] https://www.ibat.ie/wp-content/uploads/2025/03/IBAT-College-1st-Visa-2025-Fees-LATAM-EU-BRAZIL-MENA-FAR-EAST-ASIA-2.pdf -> ./wp_content/uploads/2025/03/IBAT_College_1st_Visa_2025_Fees_LATAM_EU_BRAZIL_MENA_FAR_EAST_ASIA_2.pdf\n",
      "[DOC] https://www.ibat.ie/wp-content/uploads/2024/11/v4-IBAT-077-Brochure_English-Language-Factsheet.pdf -> ./wp_content/uploads/2024/11/v4_IBAT_077_Brochure_English_Language_Factsheet.pdf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "# Setup ChromeDriver\n",
    "service = Service('/usr/bin/chromedriver')\n",
    "options = webdriver.ChromeOptions()\n",
    "# options.add_argument('--headless')  # Enable this if you want headless mode\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Constants\n",
    "BASE_URLS = [\n",
    "    \"https://www.ibat.ie/english-language-courses/learn-general-english/\",\n",
    "    \"https://www.ibat.ie/english-language-courses/ielts-prep/\"\n",
    "    \n",
    "]\n",
    "OUTPUT_DIR = \"./\"\n",
    "VISITED = set()\n",
    "\n",
    "# Ensure base directory exists\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "def sanitize_path(url):\n",
    "    parsed = urlparse(url)\n",
    "    path = parsed.path.strip(\"/\")\n",
    "    if not path or path.endswith(\"/\"):\n",
    "        path += \"index\"\n",
    "    path = path.replace(\"-\", \"_\")\n",
    "    return os.path.join(OUTPUT_DIR, path)\n",
    "\n",
    "# This was reused but the result is fine\n",
    "def normalize_url(url):\n",
    "    return urljoin(\"https://erincollege.com/\", url).split(\"#\")[0].rstrip(\"/\")\n",
    "\n",
    "def is_valid_link(href):\n",
    "    if not href:\n",
    "        return False\n",
    "    abs_url = normalize_url(href)\n",
    "    return any(abs_url.startswith(base.rstrip(\"/\")) for base in BASE_URLS)\n",
    "\n",
    "def download_document(url, save_path):\n",
    "    ext = os.path.splitext(urlparse(url).path)[1]\n",
    "    if not save_path.endswith(ext):\n",
    "        save_path += ext\n",
    "\n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        with open(save_path, \"wb\") as f:\n",
    "            f.write(r.content)\n",
    "        print(f\"[DOC] {url} -> {save_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Download failed: {url} ({e})\")\n",
    "\n",
    "def extract_markdown_from_body(body):\n",
    "    output = []\n",
    "\n",
    "    def append_text(tag, prefix=\"\"):\n",
    "        text = tag.get_text(strip=True)\n",
    "        if text:\n",
    "            output.append(f\"{prefix}{text}\")\n",
    "\n",
    "    for elem in body.descendants:\n",
    "        if elem.name in {\"script\", \"style\", \"noscript\"}:\n",
    "            continue\n",
    "        if elem.name == \"h1\":\n",
    "            append_text(elem, \"# \")\n",
    "        elif elem.name == \"h2\":\n",
    "            append_text(elem, \"## \")\n",
    "        elif elem.name == \"h3\":\n",
    "            append_text(elem, \"### \")\n",
    "        elif elem.name == \"p\":\n",
    "            append_text(elem)\n",
    "        elif elem.name == \"li\":\n",
    "            append_text(elem, \"- \")\n",
    "        elif elem.name == \"pre\":\n",
    "            code = elem.get_text().strip()\n",
    "            output.append(\"```\")\n",
    "            output.append(code)\n",
    "            output.append(\"```\")\n",
    "        elif elem.name == \"blockquote\":\n",
    "            append_text(elem, \"> \")\n",
    "\n",
    "    return \"\\n\\n\".join(output)\n",
    "\n",
    "def scrape_page(url):\n",
    "    norm_url = normalize_url(url)\n",
    "    if norm_url in VISITED:\n",
    "        return\n",
    "    VISITED.add(norm_url)\n",
    "\n",
    "    driver.get(url)\n",
    "    time.sleep(1.5)  # Let JavaScript render\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    body = soup.body\n",
    "\n",
    "    if body:\n",
    "        for tag in body([\"script\", \"style\", \"noscript\"]):\n",
    "            tag.decompose()\n",
    "        markdown_text = extract_markdown_from_body(body)\n",
    "\n",
    "        if markdown_text:\n",
    "            rel_path = sanitize_path(url) + \".txt\"\n",
    "            os.makedirs(os.path.dirname(rel_path), exist_ok=True)\n",
    "            with open(rel_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(markdown_text)\n",
    "            print(f\"[PAGE] {url} -> {rel_path}\")\n",
    "        else:\n",
    "            print(f\"No visible text found at {url}\")\n",
    "\n",
    "    # Recurse on links\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        href = a[\"href\"]\n",
    "        abs_url = urljoin(url, href)\n",
    "        norm_abs_url = normalize_url(abs_url)\n",
    "\n",
    "        if norm_abs_url in VISITED:\n",
    "            continue\n",
    "\n",
    "        if any(abs_url.lower().endswith(ext) for ext in [\".pdf\"]):\n",
    "            save_path = sanitize_path(abs_url)\n",
    "            download_document(abs_url, save_path)\n",
    "        elif is_valid_link(abs_url):\n",
    "            scrape_page(abs_url)\n",
    "\n",
    "# Start crawling\n",
    "for url in BASE_URLS:\n",
    "    scrape_page(url)\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73e7f7a6-40c4-4e84-b734-d0cbc9675122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ibat.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
